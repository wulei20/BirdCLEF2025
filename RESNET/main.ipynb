{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将音频转化为频谱图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting:   6%|▌         | 1722/28564 [10:59<1:52:50,  3.96file/s] "
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def audio_to_melspectrogram(file_path, save_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "        S_DB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "        plt.figure(figsize=(2.56, 2.56), dpi=100)\n",
    "        librosa.display.specshow(S_DB, sr=sr, cmap='magma')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout(pad=0)\n",
    "        plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error on {file_path}: {e}\")\n",
    "\n",
    "TRAIN_AUDIO_DIR = Path(\"train_audio\")            \n",
    "TRAIN_IMG_DIR   = Path(\"working/train_images\")   \n",
    "TRAIN_IMG_DIR.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "ogg_files = list(TRAIN_AUDIO_DIR.rglob(\"*.ogg\"))\n",
    "\n",
    "for audio_fp in tqdm(ogg_files, desc=\"Converting\", unit=\"file\"):\n",
    "    # 相对路径（例如 21116/iNat296867.ogg）\n",
    "    rel_fp   = audio_fp.relative_to(TRAIN_AUDIO_DIR)\n",
    "    img_fp   = TRAIN_IMG_DIR / rel_fp.with_suffix(\".png\")  # 改后缀\n",
    "    img_fp.parent.mkdir(parents=True, exist_ok=True)       # 递归建子目录\n",
    "\n",
    "    try:\n",
    "        audio_to_melspectrogram(str(audio_fp), str(img_fp))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] 处理 {audio_fp} 失败：{e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本总数: 2197\n",
      "{'image': tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1176, 0.0275],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4235, 0.0980],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.9882, 0.9294, 0.9804],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.9216, 0.7804, 0.9451],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.8941, 0.7216, 0.9294]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0353, 0.0078],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1255, 0.0275],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.5686, 0.3804, 0.7255],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.4196, 0.2902, 0.5765],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.3608, 0.2549, 0.5216]],\n",
      "\n",
      "        [[0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.1137, 0.0392],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0157, 0.3686, 0.0980],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.4000, 0.3843, 0.5333],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.3961, 0.4392, 0.4549],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.3961, 0.4588, 0.4235]]]), 'coords': tensor([ 0.0813, -0.4095]), 'target': tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, csv_path, image_root, transform=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.image_root = Path(image_root)\n",
    "        self.transform = transform\n",
    "\n",
    "        # primary_label -> idx\n",
    "        self.label2idx = {l: i for i, l in enumerate(sorted(self.df.primary_label.unique()))}\n",
    "        self.num_labels = len(self.label2idx)\n",
    "\n",
    "        self.samples = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            png_path = self.image_root / Path(row.filename).with_suffix(\".png\")\n",
    "            if not png_path.exists():\n",
    "                continue\n",
    "\n",
    "            # --------- 组装 multi‑label ---------\n",
    "            labels = [row.primary_label]\n",
    "            if pd.notna(row.secondary_labels):\n",
    "                labels += eval(row.secondary_labels)  # 列表字符串 → list\n",
    "            idxs = [self.label2idx[l] for l in labels if l in self.label2idx]\n",
    "\n",
    "            target = torch.zeros(self.num_labels, dtype=torch.float32)\n",
    "            target[idxs] = 1.0\n",
    "\n",
    "            self.samples.append(\n",
    "                dict(\n",
    "                    image_path=png_path,\n",
    "                    target=target,\n",
    "                    primary_idx=self.label2idx[row.primary_label],\n",
    "                    latitude=row.latitude if pd.notna(row.latitude) else 0.0,\n",
    "                    longitude=row.longitude if pd.notna(row.longitude) else 0.0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # 供外部调用\n",
    "    def __len_of_label__(self):\n",
    "        return self.num_labels\n",
    "\n",
    "    # -------- PyTorch API --------\n",
    "    def __len__(self):  return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "        img = Image.open(s[\"image_path\"]).convert(\"RGB\")\n",
    "        if self.transform: img = self.transform(img)\n",
    "\n",
    "        # 归一化地理坐标到 [-1,1]\n",
    "        lat = torch.tensor(s[\"latitude\"] / 90.0, dtype=torch.float32)\n",
    "        lon = torch.tensor(s[\"longitude\"] / 180.0, dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            \"image\": img,               # float tensor [3,224,224]\n",
    "            \"coords\": torch.stack([lat, lon]),  # [2]\n",
    "            \"target\": s[\"target\"],      # multi‑hot [num_labels]\n",
    "        }\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 建立数据集\n",
    "dataset = SpectrogramDataset(\n",
    "    csv_path=\"train.csv\",\n",
    "    image_root=\"working/train_images\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "sample = dataset[0]\n",
    "print('样本总数:', len(dataset))  # 样本总数\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "\n",
    "class BirdNet(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "\n",
    "        # 视觉分支：ResNet18 预训练，替换最后 fc\n",
    "        backbone = models.resnet18(weights=None)\n",
    "        state_dict = torch.load(\"resnet18-f37072fd.pth\")\n",
    "        backbone.load_state_dict(state_dict)\n",
    "        \n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-1])  # 输出 512×1×1\n",
    "        self.img_head = nn.Sequential(nn.Flatten())                     # 512\n",
    "\n",
    "        # 经纬度分支\n",
    "        self.coord_head = nn.Sequential(\n",
    "            nn.Linear(2, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # 分类器\n",
    "        self.classifier = nn.Linear(512 + 128, num_labels)\n",
    "\n",
    "    def forward(self, img, coords):\n",
    "        x_img = self.backbone(img)          # [B,512,1,1]\n",
    "        x_img = self.img_head(x_img)        # [B,512]\n",
    "\n",
    "        x_geo = self.coord_head(coords)     # [B,128]\n",
    "\n",
    "        x = torch.cat([x_img, x_geo], dim=1)  # [B,640]\n",
    "        logits = self.classifier(x)           # [B,num_labels]\n",
    "        return torch.sigmoid(logits)          # 概率 [0‑1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\X1E\\AppData\\Local\\Temp\\ipykernel_1940\\3919789208.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"resnet18-f37072fd.pth\")\n",
      "Epoch 1/5:   0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----- 数据 -----\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "ds = SpectrogramDataset(\"train.csv\", \"working/train_images\", transform)\n",
    "train_ds, val_ds = random_split(ds, [0.9, 0.1])\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# ----- 模型 -----\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "model = BirdNet(num_labels=ds.__len_of_label__()).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# ----- 训练循环 -----\n",
    "for epoch in range(NUM_EPOCHS):  # 5个 epoch\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/5\"):\n",
    "        img   = batch[\"image\"].to(device)\n",
    "        coords = batch[\"coords\"].to(device)\n",
    "        target = batch[\"target\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(img, coords)\n",
    "        loss  = criterion(preds, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # ---- 简单验证 ----\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            img, coords, target = batch[\"image\"].to(device), batch[\"coords\"].to(device), batch[\"target\"].to(device)\n",
    "            val_loss += criterion(model(img, coords), target).item() * img.size(0)\n",
    "    print(f\"Epoch {epoch+1}: val BCE={val_loss/len(val_ds):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
